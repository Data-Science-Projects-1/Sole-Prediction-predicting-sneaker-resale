{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the URLs\n",
    "\n",
    "In this notebook, we begin the process of scraping sneaker sales from the website goat.com. Because of the structure of the goat website, we first gather all the URL's for each sneaker so that we can parse through each sneaker link and gather pertinent data.\n",
    "\n",
    "I define a function that will take in a year, and get the page source for shoes for that year. I also note that I am restricting to Air Jordan's, Lifestyle, and Mens/Womens sneakers.\n",
    "\n",
    "After gathering the page sources, I get a dictionary of those hyperlinks to be used to gather the data needed to predict shoe resale price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plot\n",
    "from time import sleep\n",
    "import json, re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "\n",
    "## this is to suppress warnings I was getting in this code. \n",
    "import warnings\n",
    "# Suppress FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the GOAT sneaker website uses infinity scroll to see all sneakers,\n",
    "# I define a function that will take in a year, and get the page source for shoes for that year\n",
    "# I also note that I am restricting to Air Jordan's, Lifestyle, and Mens/Womens sneakers\n",
    "\n",
    "def GoatScraper(year):\n",
    "    goat_url = 'https://www.goat.com/sneakers?brand=air+jordan&release_date_year=' + year + '&web_groups=lifestyle&gender=men&gender=women'\n",
    "\n",
    "    ## -- --\n",
    "    ## we use selenium to scroll down the page on the website, since goat utilizes infinite scrolling\n",
    "    ## -- --\n",
    "\n",
    "    driver = webdriver.Chrome() ## utilizing the chrome browser\n",
    "    driver.maximize_window()    ## starting off with a max window to scroll\n",
    "\n",
    "    driver.get(goat_url)        ## loading the webpage \n",
    "\n",
    "    last_height = 0             ## recording the initial heigh as zero to make sure we can scroll\n",
    "                                ## the entire page\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollBy(0,8000)')    ## start off with a BIG scroll\n",
    "        sleep(1.75)                                         ## sleep, had to adjust this to reach the entire page since after\n",
    "                                                            ## you scroll the page loads, so a slower loading page needs more time to sleep\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\") ## set a new height to compare\n",
    "        \n",
    "        if new_height == last_height:   ## if we cannot scroll anymore, we break the while loop\n",
    "            print('Ended with a page height of: ' + str(new_height)) ## printing our ending page height\n",
    "            break\n",
    "        else:                           ## otherwise, we set new lower height, and continue to scroll \n",
    "            last_height = new_height\n",
    "\n",
    "    page_source = driver.page_source    ## after we scroll the page, we record the source of the page to scrape\n",
    "\n",
    "    return(page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Working on year 2013\n",
      "Ended with a page height of: 9692\n",
      "Now we are getting the links to the shoes\n",
      "--\n",
      "Done with 2013\n",
      "--\n",
      "--\n",
      "Working on year 2014\n",
      "Ended with a page height of: 11798\n",
      "Now we are getting the links to the shoes\n",
      "--\n",
      "Done with 2014\n",
      "--\n",
      "--\n",
      "Working on year 2015\n",
      "Ended with a page height of: 13052\n",
      "Now we are getting the links to the shoes\n",
      "--\n",
      "Done with 2015\n",
      "--\n",
      "--\n",
      "Working on year 2016\n",
      "Ended with a page height of: 17274\n",
      "Now we are getting the links to the shoes\n",
      "--\n",
      "Done with 2016\n",
      "--\n",
      "--\n",
      "Working on year 2017\n",
      "Ended with a page height of: 21918\n",
      "Now we are getting the links to the shoes\n",
      "--\n",
      "Done with 2017\n",
      "--\n",
      "--\n",
      "Working on year 2018\n",
      "Ended with a page height of: 38015\n",
      "Now we are getting the links to the shoes\n",
      "--\n",
      "Done with 2018\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#years = ['2019','2020','2021','2022','2023','2024']\n",
    "years = ['2013','2014','2015','2016','2017','2018']\n",
    "link_dict = {}\n",
    "\n",
    "for year in years:\n",
    "    print('--')\n",
    "    print('Working on year ' + year)\n",
    "    source = GoatScraper(year)\n",
    "    ## -- --\n",
    "    ## now we use BeatifulSoup to parse the page source for the data we want\n",
    "    ## -- --\n",
    "    soup = BeautifulSoup(source)   ## parsing the page source\n",
    "\n",
    "    ## grabbing the div labels with certain attributes. \n",
    "    ## this is gotten from the page source and seeing where the hyperlink for each shoe is kept\n",
    "    data = soup.find_all('div', attrs = {'class':\"GridStyles__GridWrapper-sc-1cm482p-1 fpmUch\"})[0]\n",
    "\n",
    "    ## under div, we need the 'a' label that contains the 'href' we are after\n",
    "    hrefs = data.find_all('a')\n",
    "    print('Now we are getting the links to the shoes')\n",
    "\n",
    "    ## we now grab those hyper links, and put them into a list to be used \n",
    "    links = list()\n",
    "    for link in hrefs:\n",
    "        links.append(link['href'])\n",
    "\n",
    "    link_dict[year] = links\n",
    "    print('--')\n",
    "    print('Done with ' + year)\n",
    "    print('--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we export the dictionary of years and url using pickle to a text file\n",
    "links_file = open('links_file', 'wb') \n",
    "pickle.dump(link_dict, links_file) \n",
    "links_file.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
